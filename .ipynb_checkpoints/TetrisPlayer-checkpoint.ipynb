{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dqn_agent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ec16cd1bb6ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdqn_agent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtetris\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTetris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatistics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dqn_agent'"
     ]
    }
   ],
   "source": [
    "from dqn_agent import DQNAgent\n",
    "from tetris import Tetris\n",
    "from datetime import datetime\n",
    "from statistics import mean, median\n",
    "import random\n",
    "from logs import CustomTensorBoard\n",
    "from tqdm import tqdm\n",
    "        \n",
    "\n",
    "# Run dqn with Tetris\n",
    "def dqn():\n",
    "    env = Tetris()\n",
    "    episodes = 2000\n",
    "    max_steps = None\n",
    "    epsilon_stop_episode = 1500\n",
    "    mem_size = 20000\n",
    "    discount = 0.95\n",
    "    batch_size = 512\n",
    "    epochs = 1\n",
    "    render_every = 50\n",
    "    log_every = 50\n",
    "    replay_start_size = 2000\n",
    "    train_every = 1\n",
    "    n_neurons = [32, 32]\n",
    "    render_delay = None\n",
    "    activations = ['relu', 'relu', 'linear']\n",
    "\n",
    "    agent = DQNAgent(env.get_state_size(),\n",
    "                     n_neurons=n_neurons, activations=activations,\n",
    "                     epsilon_stop_episode=epsilon_stop_episode, mem_size=mem_size,\n",
    "                     discount=discount, replay_start_size=replay_start_size)\n",
    "\n",
    "    log_dir = f'logs/tetris-nn={str(n_neurons)}-mem={mem_size}-bs={batch_size}-e={epochs}-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "    log = CustomTensorBoard(log_dir=log_dir)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for episode in tqdm(range(episodes)):\n",
    "        current_state = env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        if render_every and episode % render_every == 0:\n",
    "            render = True\n",
    "        else:\n",
    "            render = False\n",
    "\n",
    "        # Game\n",
    "        while not done and (not max_steps or steps < max_steps):\n",
    "            next_states = env.get_next_states()\n",
    "            best_state = agent.best_state(next_states.values())\n",
    "            \n",
    "            best_action = None\n",
    "            for action, state in next_states.items():\n",
    "                if state == best_state:\n",
    "                    best_action = action\n",
    "                    break\n",
    "\n",
    "            reward, done = env.play(best_action[0], best_action[1], render=render,\n",
    "                                    render_delay=render_delay)\n",
    "            \n",
    "            agent.add_to_memory(current_state, next_states[best_action], reward, done)\n",
    "            current_state = next_states[best_action]\n",
    "            steps += 1\n",
    "\n",
    "        scores.append(env.get_game_score())\n",
    "\n",
    "        # Train\n",
    "        if episode % train_every == 0:\n",
    "            agent.train(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "        # Logs\n",
    "        if log_every and episode and episode % log_every == 0:\n",
    "            avg_score = mean(scores[-log_every:])\n",
    "            min_score = min(scores[-log_every:])\n",
    "            max_score = max(scores[-log_every:])\n",
    "\n",
    "            log.log(episode, avg_score=avg_score, min_score=min_score,\n",
    "                    max_score=max_score)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dqn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.keras.backend' has no attribute 'get_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2457bf94f227>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_epsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_floatx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcast_to_floatx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/load_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# Try and load external backend.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Private TF Keras utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mget_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_keras_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;31m# learning_phase_scope = tf_keras_backend.learning_phase_scope  # TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mname_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.keras.backend' has no attribute 'get_graph'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, save_model, load_model\n",
    "from keras.layers import Dense\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Deep Q Learning Agent + Maximin\n",
    "#\n",
    "# This version only provides only value per input,\n",
    "# that indicates the score expected in that state.\n",
    "# This is because the algorithm will try to find the\n",
    "# best final state for the combinations of possible states,\n",
    "# in constrast to the traditional way of finding the best\n",
    "# action for a particular state.\n",
    "class DQNAgent:\n",
    "\n",
    "    '''Deep Q Learning Agent + Maximin\n",
    "\n",
    "    Args:\n",
    "        state_size (int): Size of the input domain\n",
    "        mem_size (int): Size of the replay buffer\n",
    "        discount (float): How important is the future rewards compared to the immediate ones [0,1]\n",
    "        epsilon (float): Exploration (probability of random values given) value at the start\n",
    "        epsilon_min (float): At what epsilon value the agent stops decrementing it\n",
    "        epsilon_stop_episode (int): At what episode the agent stops decreasing the exploration variable\n",
    "        n_neurons (list(int)): List with the number of neurons in each inner layer\n",
    "        activations (list): List with the activations used in each inner layer, as well as the output\n",
    "        loss (obj): Loss function\n",
    "        optimizer (obj): Otimizer used\n",
    "        replay_start_size: Minimum size needed to train\n",
    "    '''\n",
    "\n",
    "    def __init__(self, state_size, mem_size=10000, discount=0.95,\n",
    "                 epsilon=1, epsilon_min=0, epsilon_stop_episode=500,\n",
    "                 n_neurons=[32,32], activations=['relu', 'relu', 'linear'],\n",
    "                 loss='mse', optimizer='adam', replay_start_size=None):\n",
    "\n",
    "        assert len(activations) == len(n_neurons) + 1\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.memory = deque(maxlen=mem_size)\n",
    "        self.discount = discount\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / (epsilon_stop_episode)\n",
    "        self.n_neurons = n_neurons\n",
    "        self.activations = activations\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        if not replay_start_size:\n",
    "            replay_start_size = mem_size / 2\n",
    "        self.replay_start_size = replay_start_size\n",
    "        self.model = self._build_model()\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "        '''Builds a Keras deep neural network model'''\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.n_neurons[0], input_dim=self.state_size, activation=self.activations[0]))\n",
    "\n",
    "        for i in range(1, len(self.n_neurons)):\n",
    "            model.add(Dense(self.n_neurons[i], activation=self.activations[i]))\n",
    "\n",
    "        model.add(Dense(1, activation=self.activations[-1]))\n",
    "\n",
    "        model.compile(loss=self.loss, optimizer=self.optimizer)\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "    def add_to_memory(self, current_state, next_state, reward, done):\n",
    "        '''Adds a play to the replay memory buffer'''\n",
    "        self.memory.append((current_state, next_state, reward, done))\n",
    "\n",
    "\n",
    "    def random_value(self):\n",
    "        '''Random score for a certain action'''\n",
    "        return random.random()\n",
    "\n",
    "\n",
    "    def predict_value(self, state):\n",
    "        '''Predicts the score for a certain state'''\n",
    "        return self.model.predict(state)[0]\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        '''Returns the expected score of a certain state'''\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        if random.random() <= self.epsilon:\n",
    "            return self.random_value()\n",
    "        else:\n",
    "            return self.predict_value(state)\n",
    "\n",
    "\n",
    "    def best_state(self, states):\n",
    "        '''Returns the best state for a given collection of states'''\n",
    "        max_value = None\n",
    "        best_state = None\n",
    "\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.choice(list(states))\n",
    "\n",
    "        else:\n",
    "            for state in states:\n",
    "                value = self.predict_value(np.reshape(state, [1, self.state_size]))\n",
    "                if not max_value or value > max_value:\n",
    "                    max_value = value\n",
    "                    best_state = state\n",
    "\n",
    "        return best_state\n",
    "\n",
    "\n",
    "    def train(self, batch_size=32, epochs=3):\n",
    "        '''Trains the agent'''\n",
    "        n = len(self.memory)\n",
    "    \n",
    "        if n >= self.replay_start_size and n >= batch_size:\n",
    "\n",
    "            batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "            # Get the expected score for the next states, in batch (better performance)\n",
    "            next_states = np.array([x[1] for x in batch])\n",
    "            next_qs = [x[0] for x in self.model.predict(next_states)]\n",
    "\n",
    "            x = []\n",
    "            y = []\n",
    "\n",
    "            # Build xy structure to fit the model in batch (better performance)\n",
    "            for i, (state, _, reward, done) in enumerate(batch):\n",
    "                if not done:\n",
    "                    # Partial Q formula\n",
    "                    new_q = reward + self.discount * next_qs[i]\n",
    "                else:\n",
    "                    new_q = reward\n",
    "\n",
    "                x.append(state)\n",
    "                y.append(new_q)\n",
    "\n",
    "            # Fit the model to the given values\n",
    "            self.model.fit(np.array(x), np.array(y), batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "\n",
    "            # Update the exploration variable\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon -= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-eafdefdb3491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCustomTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "from tensorflow.summary import FileWriter\n",
    "\n",
    "class CustomTensorBoard(TensorBoard):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.writer = FileWriter(self.log_dir)\n",
    "\n",
    "    def set_model(self, model):\n",
    "        pass\n",
    "\n",
    "    def log(self, step, **stats):\n",
    "        self._write_logs(stats, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
